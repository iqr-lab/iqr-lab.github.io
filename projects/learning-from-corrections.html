<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  


























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Learning from Corrections | IQR Lab</title>

<link rel="icon" href="/images/icon.png">

<meta name="title" content="Learning from Corrections">
<meta name="description" content="Inquisitive Robotics Lab at Yale. Scalable, multicamera distributed system for realtime pointcloud stitching in IQR Lab. This program is currently designed to use the D400 Series Intel RealSense depth cameras. Using the librealsense 2.0 SDK, depth frames are grabbed and pointclouds are computed on the edge, before sending the raw XYZRGB values to a central computer over a TCP sockets. The central program stitches the pointclouds together and displays it a viewer using PCL libraries.">

<meta property="og:title" content="Learning from Corrections">
<meta property="og:site_title" content="IQR Lab">
<meta property="og:description" content="Inquisitive Robotics Lab at Yale. Scalable, multicamera distributed system for realtime pointcloud stitching in IQR Lab. This program is currently designed to use the D400 Series Intel RealSense depth cameras. Using the librealsense 2.0 SDK, depth frames are grabbed and pointclouds are computed on the edge, before sending the raw XYZRGB values to a central computer over a TCP sockets. The central program stitches the pointclouds together and displays it a viewer using PCL libraries.">
<meta property="og:url" content="">
<meta property="og:image" content="/images/share.jpg">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="Learning from Corrections">
<meta property="twitter:description" content="Inquisitive Robotics Lab at Yale. Scalable, multicamera distributed system for realtime pointcloud stitching in IQR Lab. This program is currently designed to use the D400 Series Intel RealSense depth cameras. Using the librealsense 2.0 SDK, depth frames are grabbed and pointclouds are computed on the edge, before sending the raw XYZRGB values to a central computer over a TCP sockets. The central program stitches the pointclouds together and displays it a viewer using PCL libraries.">
<meta property="twitter:url" content="">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/images/share.jpg">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "Learning from Corrections",
    "description": "Inquisitive Robotics Lab at Yale. Scalable, multicamera distributed system for realtime pointcloud stitching in IQR Lab. This program is currently designed to use the D400 Series Intel RealSense depth cameras. Using the librealsense 2.0 SDK, depth frames are grabbed and pointclouds are computed on the edge, before sending the raw XYZRGB values to a central computer over a TCP sockets. The central program stitches the pointclouds together and displays it a viewer using PCL libraries.",
    "headline": "Learning from Corrections",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/images/icon.png" }
    },
    "url": ""
  }
</script>

<link rel="alternate" type="application/rss+xml" href="/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.3.0/css/all.css" rel="preload" as="style" onload="this.onload = null; this.rel = 'stylesheet';">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.3.0/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/_styles/all.css" rel="stylesheet">
  

  
    <link href="/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/_styles/background.css" rel="stylesheet">
  

  
    <link href="/_styles/body.css" rel="stylesheet">
  

  
    <link href="/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/_styles/button.css" rel="stylesheet">
  

  
    <link href="/_styles/card.css" rel="stylesheet">
  

  
    <link href="/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/_styles/code.css" rel="stylesheet">
  

  
    <link href="/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/_styles/float.css" rel="stylesheet">
  

  
    <link href="/_styles/font.css" rel="stylesheet">
  

  
    <link href="/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/_styles/form.css" rel="stylesheet">
  

  
    <link href="/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/_styles/header.css" rel="stylesheet">
  

  
    <link href="/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/_styles/image.css" rel="stylesheet">
  

  
    <link href="/_styles/link.css" rel="stylesheet">
  

  
    <link href="/_styles/list.css" rel="stylesheet">
  

  
    <link href="/_styles/main.css" rel="stylesheet">
  

  
    <link href="/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/_styles/section.css" rel="stylesheet">
  

  
    <link href="/_styles/table.css" rel="stylesheet">
  

  
    <link href="/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/_styles/util.css" rel="stylesheet">
  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/_scripts/anchors.js"></script>

  <script src="/_scripts/dark-mode.js"></script>

  <script src="/_scripts/fetch-tags.js"></script>

  <script src="/_scripts/search.js"></script>

  <script src="/_scripts/site-search.js"></script>

  <script src="/_scripts/tooltip.js"></script>


</head>

  <body>
    







<header class="background" style="--image: url('/images/PXL_20230928_151703139.jpg')" data-dark="true">
  <a href="/" class="home">
    
      <span class="logo">
        
          <?xml version="1.0" encoding="UTF-8"?><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 358.77 304.28"><defs><style>.cls-1{fill:#00356b;stroke:#fff;stroke-miterlimit:10;stroke-width:4px;}</style></defs><path class="cls-1" d="M22.27,269.98v16.8h220.78v-16.8c0-13.28-10.77-24.05-24.05-24.05H46.33c-13.28,0-24.05,10.77-24.05,24.05h0Z"></path><path class="cls-1" d="M102.93,165.27c-6.68,0-13.03-1.46-18.73-4.09l21.32,70.79h118.92l-79.53-95.51c-6.51,16.84-22.87,28.81-41.97,28.81h0Z"></path><path class="cls-1" d="M70.88,120.29c0,17.71,14.35,32.06,32.06,32.06s32.06-14.35,32.06-32.06-14.35-32.06-32.06-32.06-32.06,14.35-32.06,32.06h0Z"></path><path class="cls-1" d="M231.68,18.43c-14.28,0-25.85,11.57-25.85,25.85s11.57,25.85,25.85,25.85,25.85-11.57,25.85-25.85-11.57-25.85-25.85-25.85h0Z"></path><path class="cls-1" d="M102.93,75.3c22.9,0,41.86,17.2,44.63,39.37l56.35-43.35c-6.81-7-11.01-16.54-11.01-27.05,0-4.35.72-8.54,2.05-12.45l-96.04,43.65c1.32-.12,2.66-.18,4.02-.18h0Z"></path><path class="cls-1" d="M240.72,81.99l21.38,20.52c6.77-9.28,17.73-15.32,30.08-15.32h.1l-23.04-33.23c-3.58,13.83-14.6,24.69-28.52,28.02h0Z"></path><path class="cls-1" d="M292.18,100.12c-13.42,0-24.3,10.88-24.3,24.3,0,5.23,1.66,10.07,4.47,14.03l-20.59,16.28c-2.88,2.28-4.63,5.69-4.8,9.35-.17,3.67,1.25,7.22,3.91,9.76l17.58,16.78c1.25,1.2,2.86,1.79,4.46,1.79,1.7,0,3.41-.67,4.68-2,2.46-2.58,2.37-6.67-.21-9.14l-17.37-16.58,22.66-17.91c2.92,1.25,6.14,1.94,9.52,1.94s6.53-.68,9.43-1.9l22.61,17.87-17.37,16.58c-2.58,2.46-2.68,6.56-.21,9.14,1.27,1.33,2.97,2,4.68,2,1.6,0,3.21-.59,4.46-1.79l17.58-16.78c2.66-2.53,4.08-6.09,3.91-9.76-.17-3.67-1.92-7.08-4.8-9.35l-20.5-16.2c2.84-3.98,4.52-8.85,4.52-14.11,0-13.42-10.88-24.3-24.3-24.3h0Z"></path></svg>
        
      </span>
    
    
      <span class="title" data-tooltip="Home">
        
          <span>IQR Lab</span>
        
        
          <span>Inquisitive Robotics Lab at Yale</span>
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/research/" data-tooltip="Published works">
          Research
        </a>
      
    
      
        <a href="/projects/" data-tooltip="Software, datasets, and more">
          Projects
        </a>
      
    
      
        <a href="/docs/" data-tooltip="Internal Documentation and Resources">
          Docs
        </a>
      
    
      
        <a href="/team/" data-tooltip="About our team">
          Team
        </a>
      
    
      
        <a href="/blog/" data-tooltip="Musings and miscellany">
          Blog
        </a>
      
    
      
        <a href="/contact/" data-tooltip="Email, address, and location">
          Contact
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - wrap each table in div to allow for scrolling
  - filter out blank sections
-->








  
  
  

  <section class="background" data-size="page">
    <h1 class="center">Learning from Corrections</h1>

<div class="post-info">
  

  
  

  

  
</div>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->


<p>With the continued integration of machines into our everyday lives, the principle of non-technical human teachers being able to effectively communicate with and efficiently train robots becomes increasingly relevant. Current and prior research has looked into how people can train a robot to complete manipulation-based tasks using different modalities or interaction types, such as demonstrations [1] and ranked preferences [2]. Alternatively, a person can monitor a robot as it attempts to complete a task, interceding to provide a \emph{correction} when they deem it necessary to modify the robot’s behavior [3,4]. For example, if a robot that is supposed to pick up a mug from the table is moving away from the table instead, a human teacher may offer assistance by correcting the robot’s motion and pushing it in the right direction. This correction should inform how the robot behaves in future variations of the task, while also implying how the robot should <strong>not</strong> behave (i.e., the behavior that prompted the teacher to intercede in the first place).</p>

<p>From a physical human-robot interaction perspective, corrections are a natural method for collaboration and communication between humans and robots [5]. For non-technical users, it is easier to express their intent by directly interacting with the robots, as opposed to programming them. Furthermore, corrections put the human in the role of a supervisor rather than teacher, where they only step in as needed rather than try to teach the robot from scratch. This reduces the amount of data required to improve task performance and increases the efficiency of robot learning [6].</p>

<p>From a ML perspective, corrections have the potential to provide rich information for a robot to learn an optimal model of the task objective. Based on the initial behavior of the robot, a human’s correction of that behavior can indicate what the robot did right or wrong, and how to avoid making similar mistakes in the future. However, corrections are complicated to interpret. Bayesian Inverse Reinforcement Learning (BIRL) provides a method for learning a reward function that maximizes the likelihood of the teacher’s feedback [7, 8, 9]. Yet, this approach requires that we have a model of how the human feedback is influenced by the task objectives. When learning from corrections, there are other conflating influences such as the <strong>abruptness</strong> of the corrections (caused by the human’s binary decision of whether or not to correct the robot’s behavior) and the subjective <strong>bias</strong> that influences the teacher’s belief over whether or not the robot will succeed at the task. This decision to interrupt and modify the robot’s motion may occur after the robot has made a mistake or in anticipation of a future mistake that has not even occurred yet, and may be biased by the robot’s previous behavior [10]. Prior work has focused on interpreting and learning from corrections [11, 12, 13], but has not looked into how the teacher decides to intervene and provide a correction in the first place.</p>

<p>The key challenge we address is how to isolate the influence of task objectives on corrections from other conflating influences. To do this, we need to first model the effects of these influences on corrections.</p>

<p>In this abstract, we hypothesize the effects of three variables (legibility, task tolerance, and physical effort) on corrections and formalize them as predictive models. We then propose a user study to evaluate these models. Finally, we discuss how these models can be used to improve ML algorithms in future work.</p>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->


<div class="post-nav">
  <span>
    
  </span>
  <span>
    
  </span>
</div>
  </section>


    </main>
    


<footer class="background" style="--image: url('/images/iqr_background.png')" data-dark="true" data-size="wide">
  <!--
    <div>
      Extra details like contact info or address
    </div>
  -->

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="mailto:tesca.fitzgerald@yale.edu" data-tooltip="Email" data-style="bare" aria-label="Email">
      <i class="icon fa-solid fa-envelope"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://orcid.org/0000-0003-0867-0546" data-tooltip="ORCID" data-style="bare" aria-label="ORCID">
      <i class="icon fa-brands fa-orcid"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://scholar.google.com/citations?user=UTmj6K4AAAAJ" data-tooltip="Google Scholar" data-style="bare" aria-label="Google Scholar">
      <i class="icon fa-brands fa-google"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://github.com/iqr-lab" data-tooltip="GitHub" data-style="bare" aria-label="GitHub">
      <i class="icon fa-brands fa-github"></i>
      
    </a>
  </div>


    
  </div>

  <div>
    © 2024
    IQR Lab
      |   Built with
    <a href="https://github.com/greenelab/lab-website-template">
      Lab Website Template
    </a>
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
